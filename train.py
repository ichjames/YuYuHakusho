import os
import sys
import math

import retro
from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.vec_env import SubprocVecEnv
from custYuYuHakushoWrapper import CustYuYuHakushoWrapper


'''
æ—¥å¿—å‚æ•°è¯´æ˜ï¼š

1. rollout/ â€”â€” ç¯å¢ƒäº¤äº’é˜¶æ®µï¼ˆRollout Phaseï¼‰
è¿™æ˜¯æŒ‡æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­è¿è¡Œã€æ”¶é›†ç»éªŒï¼ˆçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ç­‰ï¼‰çš„è¿‡ç¨‹ã€‚
ep_len_mean	æ¯ä¸ª episodeï¼ˆå›åˆï¼‰å¹³å‡æŒç»­äº† 566 æ­¥ã€‚è¿™åæ˜ ä»»åŠ¡çš„å¹³å‡å®Œæˆæ—¶é—´æˆ–ç”Ÿå­˜æ—¶é—´ã€‚æ•°å€¼ç¨³å®šè¯´æ˜æ¢ç´¢è¾ƒä¸€è‡´ã€‚
ep_rew_mean	æ¯ä¸ª episode çš„å¹³å‡ç´¯ç§¯å¥–åŠ±ä¸º -971ã€‚è´Ÿå€¼è¯´æ˜ï¼šè¦ä¹ˆä»»åŠ¡æœ¬èº«æƒ©ç½šè¾ƒå¤šï¼ˆå¦‚æ§åˆ¶ä»»åŠ¡ä¸­åç¦»ç›®æ ‡å—ç½šï¼‰ï¼Œè¦ä¹ˆæ¨¡å‹å°šæœªå­¦ä¼šæœ‰æ•ˆç­–ç•¥ã€‚éœ€è¦ç»“åˆå…·ä½“ä»»åŠ¡åˆ¤æ–­æ˜¯å¦æ­£å¸¸ã€‚

2. time/ â€”â€” æ—¶é—´ä¸è®­ç»ƒè¿›åº¦
fps	  Frames Per Secondï¼Œæ¯ç§’å¤„ç†çš„ç¯å¢ƒæ­¥æ•°ï¼ˆtimestepsï¼‰ï¼Œåæ˜ è®­ç»ƒæ•ˆç‡ã€‚è¶Šé«˜è¶Šå¥½ï¼ˆè¯´æ˜é‡‡æ ·+è®­ç»ƒé€Ÿåº¦å¿«ï¼‰ã€‚354 å±äºä¸­ç­‰åä¸Šæ°´å¹³ã€‚
iterations	å½“å‰å·²å®Œæˆ 17 è½® PPO æ›´æ–°è¿­ä»£ã€‚æ¯è½®é€šå¸¸åŒ…æ‹¬ï¼šé‡‡æ ·ä¸€æ‰¹æ•°æ® â†’ å¤šæ¬¡ä¼˜åŒ–æ›´æ–°ç­–ç•¥ç½‘ç»œã€‚
time_elapsed	å·²è¿‡å» 393 ç§’ â‰ˆ 6.5 åˆ†é’Ÿã€‚ä»è®­ç»ƒå¼€å§‹åˆ°ç°åœ¨çš„æ€»è€—æ—¶ï¼ˆç§’ï¼‰ã€‚
total_timesteps	 åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ™ºèƒ½ä½“æ€»å…±ä¸ç¯å¢ƒäº¤äº’äº† 139,264 æ­¥ã€‚è¿™æ˜¯è¡¡é‡è®­ç»ƒé‡çš„æ ¸å¿ƒæŒ‡æ ‡ã€‚


3. train/ â€”â€” è®­ç»ƒé˜¶æ®µï¼ˆPolicy Updateï¼‰
è¿™éƒ¨åˆ†æ¥è‡ªç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ï¼Œé€šå¸¸åŸºäº rollout æ”¶é›†çš„æ•°æ®è¿›è¡Œå¤šè½®æ¢¯åº¦æ›´æ–°ã€‚
approx_kl	Approximate KL divergenceï¼ˆè¿‘ä¼¼KLæ•£åº¦ï¼‰ï¼Œè¡¡é‡æ–°æ—§ç­–ç•¥ä¹‹é—´çš„å·®å¼‚ã€‚PPO ç”¨å®ƒæ¥åˆ¤æ–­æ˜¯å¦â€œèµ°å¾—å¤ªè¿œâ€ã€‚<br>âœ… ä¸€èˆ¬ < 0.01 æ˜¯å¥½çš„ï¼›> 0.01~0.02 å¯èƒ½æç¤ºç­–ç•¥å˜åŒ–å¤ªå¤§ï¼Œå¯è€ƒè™‘è§¦å‘ early stoppingã€‚<br>ğŸ‘‰ å½“å‰å€¼ 0.0085 æ˜¯å¥åº·çš„ã€‚
clip_fraction	è¡¨ç¤ºåœ¨ PPO çš„ clip æœºåˆ¶ä¸­ï¼Œæœ‰ 26.4% çš„æ¢¯åº¦æ›´æ–°è¢«è£å‰ªäº†ã€‚<br>ç†æƒ³å€¼åœ¨ 0.1~0.5 ä¹‹é—´ã€‚å¤ªä½è¯´æ˜ clip_range å¤ªå¤§ï¼Œå¤ªé«˜è¯´æ˜å¤ªå°ã€‚<br>ğŸ‘‰ 0.264 æ˜¯åˆç†èŒƒå›´ï¼Œè¯´æ˜ clip è®¾ç½®å¾—å½“ã€‚
clip_range	PPO çš„ clipping èŒƒå›´ï¼ˆÎµï¼‰ï¼Œå³ç­–ç•¥æ›´æ–°çš„â€œä¿¡ä»»åŒºåŸŸâ€åŠå¾„ã€‚<br>å¸¸è§åˆå§‹å€¼ä¸º 0.1~0.2ã€‚å¦‚æœæ˜¯å›ºå®šå€¼åˆ™ä¸å˜ï¼›è‹¥è‡ªé€‚åº”è°ƒæ•´ï¼Œè¿™é‡Œä¼šå˜åŒ–ã€‚
entropy_loss	ç­–ç•¥ç†µçš„è´Ÿå€¼ï¼ˆå®é™…æ˜¯ entropy çš„æŸå¤±é¡¹ï¼‰ã€‚<br>ç†µè¶Šé«˜ï¼Œç­–ç•¥è¶Šéšæœºï¼ˆæ¢ç´¢æ€§å¼ºï¼‰ï¼›å¤ªä½ä¼šå¯¼è‡´è¿‡æ—©æ”¶æ•›ã€‚<br>ğŸ‘‰ -7.86 çš„ç»å¯¹å€¼è¾ƒå¤§ï¼Œè¯´æ˜ç­–ç•¥ä»æœ‰è¾ƒå¼ºæ¢ç´¢æ€§ï¼ˆå¯èƒ½æ˜¯å¥½ç°è±¡ï¼Œå°¤å…¶åœ¨æ—©æœŸï¼‰ã€‚æ³¨æ„ï¼šæœ‰äº›æ¡†æ¶è®°å½•ä¸º -entropyï¼Œæ‰€ä»¥æ˜¯è´Ÿçš„ã€‚
explained_variance	è¯„ä¼°å€¼å‡½æ•°ï¼ˆValue Networkï¼‰å¯¹å›æŠ¥ï¼ˆreturnï¼‰çš„æ‹Ÿåˆç¨‹åº¦ã€‚<br>å…¬å¼ï¼š1 - Var(y - Å·)/Var(y)<br>ğŸ‘‰ 0.271 åä½ï¼Œè¯´æ˜ value network è¿˜ä¸èƒ½å¾ˆå¥½é¢„æµ‹æœªæ¥å›æŠ¥ï¼ˆæœ‰å¾…æå‡ï¼‰ã€‚<br>ğŸ¯ ç†æƒ³æ¥è¿‘ 1.0ï¼›ä½äº 0 æˆ–æ¥è¿‘ 0 è¡¨ç¤ºæ‹Ÿåˆå·®ã€‚
learning_rate	å½“å‰å­¦ä¹ ç‡ã€‚PPO å¸¸ç”¨ Adam ä¼˜åŒ–å™¨ï¼Œåˆå§‹ lr é€šå¸¸ä¸º 3e-4ï¼Œè¿™é‡Œç•¥ä½ï¼Œå¯èƒ½æ˜¯å­¦ä¹ ç‡è¡°å‡åçš„å€¼ã€‚
loss	æ€»æŸå¤±ï¼ˆå¯èƒ½æ˜¯ value_loss + policy_loss + entropy_loss çš„åŠ æƒå’Œï¼‰ï¼Œç”¨äºåå‘ä¼ æ’­ã€‚<br>è¿™ä¸ªå€¼æœ¬èº«æ„ä¹‰ä¸å¤§ï¼Œå…³é”®æ˜¯çœ‹å®ƒæ˜¯å¦ç¨³å®šä¸‹é™ã€‚125 åé«˜ï¼Œä½†è¦ç»“åˆä»»åŠ¡å¤æ‚åº¦çœ‹è¶‹åŠ¿ã€‚
n_updates	å½“å‰å·²å®Œæˆ 64 æ¬¡å‚æ•°æ›´æ–°ï¼ˆå¯èƒ½æ˜¯æ¯æ¬¡ rollout ååšå¤šæ¬¡ minibatch æ›´æ–°ï¼‰ã€‚
policy_gradient_loss	PPO çš„ç­–ç•¥æ¢¯åº¦æŸå¤±ï¼ˆé€šå¸¸æ˜¯ clipped surrogate lossï¼‰ã€‚<br>å€¼è¾ƒå°æ˜¯æ­£å¸¸çš„ï¼Œå…³é”®æ˜¯çœ‹å…¶ä¸‹é™è¶‹åŠ¿ã€‚å½“å‰ 0.009 æ˜¯åˆç†èŒƒå›´ã€‚
value_loss	å€¼å‡½æ•°ï¼ˆCriticï¼‰çš„æŸå¤±ï¼Œé€šå¸¸æ˜¯å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼š(estimated_value - actual_return)Â²ã€‚<br>182 åé«˜ï¼Œç»“åˆ explained_variance=0.271ï¼Œè¯´æ˜ critic æ‹Ÿåˆæ•ˆæœè¾ƒå·®ï¼Œå¯èƒ½éœ€è¦ï¼š<br>- æ›´å¤§ç½‘ç»œå®¹é‡<br>- æ›´å¤šè®­ç»ƒ epochs<br>- æ›´å¥½çš„ GAE å‚æ•°ï¼ˆå¦‚ Î³, Î»ï¼‰
'''

Total_Timesteps = 3000000
NUM_ENV = 16
LOG_DIR = 'logs'
os.makedirs(LOG_DIR, exist_ok=True)

'''d
è¯¥å‡½æ•°å®šä¹‰äº†ä¸€ä¸ªçº¿æ€§è°ƒåº¦å™¨ï¼ŒåŠŸèƒ½å¦‚ä¸‹ï¼š

å°†åˆå§‹å€¼å’Œæœ€ç»ˆå€¼è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œå¹¶ç¡®ä¿åˆå§‹å€¼å¤§äº0ã€‚
è¿”å›ä¸€ä¸ªå†…éƒ¨å‡½æ•°schedulerï¼Œæ ¹æ®è¿›åº¦progressï¼ˆ0åˆ°1ä¹‹é—´ï¼‰çº¿æ€§æ’å€¼è®¡ç®—å½“å‰å€¼ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼šcurrent_value = final_value + progress * (initial_value - final_value)ã€‚
'''
# Linear scheduler
def linear_schedule(initial_value, final_value=0.0):

    if isinstance(initial_value, str):
        initial_value = float(initial_value)
        final_value = float(final_value)
        assert (initial_value > 0.0)

    def scheduler(progress):
        return final_value + progress * (initial_value - final_value)

    return scheduler

'''
è¯¥Pythonå‡½æ•°make_envç”¨äºåˆ›å»ºä¸€ä¸ªå¤å¤æ¸¸æˆç¯å¢ƒï¼Œå¹¶è¿›è¡Œç›‘æ§å’Œç§å­è®¾ç½®ï¼š

1æ¥å—æ¸¸æˆåã€çŠ¶æ€å’Œç§å­å‚æ•°ã€‚
2é€šè¿‡retro.makeåˆ›å»ºç¯å¢ƒï¼Œé™åˆ¶åŠ¨ä½œå¹¶è®¾ç½®å›¾åƒè§‚å¯Ÿç±»å‹ã€‚
3å°è£…ç¯å¢ƒè‡³Monitorä»¥è®°å½•æ€§èƒ½ã€‚
4è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒå¯é‡å¤ã€‚
5è¿”å›å‡†å¤‡å¥½çš„ç¯å¢ƒã€‚
'''
def make_env(game, state, seed=0):
    def _init():
        env = retro.make(
            game=game, 
            state=state, 
            use_restricted_actions=retro.Actions.FILTERED, 
            obs_type=retro.Observations.IMAGE    
        )

        env = CustYuYuHakushoWrapper(env)
        env = Monitor(env, 'logs/')
        env.seed(seed)
        return env
    return _init

# DoubleDragonIITheRevenge-Nes
# BalloonFight-Nes
# StreetFighterIISpecialChampionEdition-Genesis

def main():
    game = "YuYuHakusho-Genesis"
    state = "Level2.state"
    env = SubprocVecEnv([make_env(game, state=state, seed=i) for i in range(NUM_ENV)])

    # Set linear schedule for learning rate
    # Start
    lr_schedule = linear_schedule(2.5e-4, 2.5e-6)

    # fine-tune
    # lr_schedule = linear_schedule(5.0e-5, 2.5e-6)

    # Set linear scheduler for clip range
    # Start
    clip_range_schedule = linear_schedule(0.15, 0.025)

    # fine-tune
    # clip_range_schedule = linear_schedule(0.075, 0.025)

    model = PPO(
        "CnnPolicy", 
        env,
        device="cuda", 
        verbose=1,
        n_steps=512,
        batch_size=512,
        n_epochs=4,
        gamma=0.94,
        learning_rate=lr_schedule,
        clip_range=clip_range_schedule,
        tensorboard_log="logs"
    )

    # Set the save directory
    save_dir = "trained_models"
    os.makedirs(save_dir, exist_ok=True)

    # Load the model from file
    # model_path = "trained_models/ppo_yuyuhakusho_final_base.zip"
    
    # Load model and modify the learning rate and entropy coefficient
    # custom_objects = {
    #     "learning_rate": lr_schedule,
    #     "clip_range": clip_range_schedule,
    #     "n_steps": 512
    # }
    # model = PPO.load(model_path, env=env, device="cuda", custom_objects=custom_objects)

    # Set up callbacks
    # Note that 1 timesetp = 6 frame
    checkpoint_interval = 100000 / NUM_ENV # checkpoint_interval * num_envs = total_steps_per_checkpoint
    checkpoint_callback = CheckpointCallback(save_freq=checkpoint_interval, save_path=save_dir, name_prefix="ppo_yuyuhakusho")

    # Writing the training logs from stdout to a file
    original_stdout = sys.stdout
    log_file_path = os.path.join(save_dir, "training_log.txt")
    with open(log_file_path, 'w') as log_file:
        sys.stdout = log_file
    
        model.learn(
            total_timesteps=int(Total_Timesteps), # total_timesteps = stage_interval * num_envs * num_stages (1120 rounds)
            callback=[checkpoint_callback], # stage_increase_callback]
        )
        env.close()

    # Restore stdout
    sys.stdout = original_stdout

    # Save the final model
    model.save(os.path.join(save_dir, "ppo_yuyuhakusho_final.zip"))

if __name__ == "__main__":
    main()