# Custom environment wrapper
from datetime import datetime
import gym
import time
import math
import numpy as np
import collections

# 动作编号对应含义（共 12 个动作）
ACTION_MEANING = {
    0: "NO_OP",
    1: "LEFT",
    2: "RIGHT",
    3: "UP",
    4: "DOWN",
    5: "LIGHT_PUNCH",      # 轻拳
    6: "HEAVY_PUNCH",      # 重拳
    7: "LIGHT_KICK",       # 轻脚
    8: "HEAVY_KICK",       # 重脚
    9: "BLOCK",            # 防御
    10: "SKILL_A",         # 技能A（如灵光弹）
    11: "SKILL_B"          # 技能B（如灵光波动拳）
}

ATTACK_ACTIONS = [5, 6, 7, 8, 10, 11]
MOVE_ACTIONS = [1, 2, 3, 4]
SKILL_ACTIONS = [10, 11]
NO_OP = 0
BLOCK = 9

def normalize(min_num, max_num, num):
    return (num - min_num) / (max_num - min_num) * 100

COST_TOTALSCORELIANXUCOUNT = 10

class CustYuYuHakushoWrapper(gym.Wrapper):

    def __init__(self, env, reset_round=True, rendering=False,limit_step=True):
        super(CustYuYuHakushoWrapper, self).__init__(env)
        self.env = env

        # Use a deque to store the last 9 frames
        self.num_frames = 9
        self.frame_stack = collections.deque(maxlen=self.num_frames)
        self.num_step_frames = 6
        self.reward_coeff = 3.0
        self.action_space = gym.spaces.Discrete(12)
        # 注意,这里的shape为 112,120,3 
        # 怎么得出此值的？ 因为这个游戏的画面输出是 224*320，我们的目的是隔行取值（用于减小运算的图像数据量）。所以输出的图像为 112*160
        # 这里的shape的值，直接影响到reset时，返回np.stack()的维度。需要特别注意
        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(112, 160, 3), dtype=np.uint8)

        self.reset_round = reset_round
        self.rendering = rendering
        
        self.full_hp = 223
        self.full_sp = 223
        self.prev_player_health = self.full_hp
        self.prev_oppont_health = self.full_hp
        self.prev_player_sp = self.full_sp
        self.prev_oppont_sp = self.full_sp

        self.totalScoreLianXuCount = 0  #连续得分
        self.COST_SWITCH_LIANXU = False

    def _stack_observation(self):
        return np.stack([self.frame_stack[i * 3 + 2][:, :, i] for i in range(3)], axis=-1)
    
    def reset(self):
        observation = self.env.reset()
        
        self.prev_player_health = self.full_hp
        self.prev_oppont_health = self.full_hp
        self.prev_player_sp = self.full_sp
        self.prev_oppont_sp = self.full_sp

        # 这里处理的是隔行取值（每一帧图像，取隔行隔列的像素值）用于减小图像数据的运算量
        self.frame_stack.clear()
        for _ in range(self.num_frames):
            self.frame_stack.append(observation[::2, ::2, :])

        # print("线程{}：执行reset，已重置".format(threading.get_ident()))

        # 这里是将3帧（隔行取值后的图像）图像堆叠成为一张完整的RGB图像，用于后续的计算。
        return np.stack([self.frame_stack[i * 3 + 2][:, :, i] for i in range(3)], axis=-1)

    def _action_to_retro_array(self, action):
        # 假设你有 12 个按钮：A, B, C, X, Y, Z, Up, Down, Left, Right, Start, Mode
        num_buttons = self.env.num_buttons  # 通常是 12
        buttons = [0] * num_buttons
        
        # 定义你的动作映射
        if action == 0:
            pass  # NOOP
        elif action == 1:
            buttons[8] = 1  # LEFT  → 左 (index 8)
        elif action == 2:
            buttons[9] = 1  # RIGHT → 右 (index 9)
        elif action == 3:
            buttons[6] = 1  # UP
        elif action == 4:
            buttons[7] = 1  # DOWN
        elif action == 5:
            buttons[0] = 1  # LIGHT_PUNCH → A
        elif action == 6:
            buttons[1] = 1  # HEAVY_PUNCH → B
        elif action == 7:
            buttons[2] = 1  # LIGHT_KICK → C
        elif action == 8:
            buttons[3] = 1  # HEAVY_KICK → X
        elif action == 9:
            buttons[7] = 1  # BLOCK → DOWN
        elif action == 10:
            # 技能1：↓↘→ + A（简化为 → + A）
            buttons[9] = 1
            buttons[0] = 1
        elif action == 11:
            # 技能2：↓↙← + B
            buttons[8] = 1
            buttons[1] = 1
        
        return buttons
    def step(self, action):
          
        # 把你的动作编号（如 5）转成 retro 所需的按钮数组
        retro_action = self._action_to_retro_array(action)
        # 执行原始环境的步骤
        observation, reward, done, info = self.env.step(retro_action)
        self.frame_stack.append(observation[::2, ::2, :])

        custom_done = False
        custom_reward = 0.0

        # Render the game if rendering flag is set to True.
        if self.rendering:
            self.env.render()
            time.sleep(0.005)
        
        for _ in range(self.num_step_frames - 1):
            # Keep the button pressed for (num_step_frames - 1) frames.
            obs, _reward, _done, info = self.env.step(retro_action)
            self.frame_stack.append(obs[::2, ::2, :])
            if self.rendering:
                self.env.render()
                time.sleep(0.005)
      

        # 敌我双方的HP和SP
        curr_agent_hp = info['agent_hp']
        curr_agent_sp = info['agent_sp']
        curr_enemy_hp = info['enemy_hp']
        curr_enemy_sp = info['enemy_sp']

        # Game is over and player loses.
        if curr_agent_hp < 0:
            custom_reward = -math.pow(self.full_hp, (curr_enemy_hp + 1) / (self.full_hp + 1))    
            custom_done = True

            print(f"Trainning: Game is over and player loses. custom_reward={custom_reward}")

        # Game is over and player wins.
        elif curr_enemy_hp < 0:
            custom_reward = math.pow(self.full_hp, (curr_agent_hp + 1) / (self.full_hp + 1)) * (self.reward_coeff * 0.5)
            custom_done = True

            print(f"Trainning: Game is over and player wins. custom_reward={custom_reward}")

        # While the fighting is still going on
        else:
            makeDamageReward = self.reward_coeff * (self.prev_oppont_health - curr_enemy_hp) - (self.prev_player_health - curr_agent_hp)

            # 2. 过程奖励（密集）
            damage_dealt = max(0, self.prev_oppont_health - curr_enemy_hp)
            damage_taken = max(0, self.prev_player_health - curr_agent_hp)

            custom_reward += damage_dealt * 1.0
            custom_reward -= damage_taken * 1.2

            # # 3. 动作合理性
            # if action in ATTACK_ACTIONS:
            #     if damage_dealt > 0:
            #         custom_reward += 0.3  # 成功命中奖励
            #     else:
            #         custom_reward -= 0.2  # 打空惩罚

            # # 4. 避免“什么都不做”——轻微惩罚
            # if action == NO_OP:
            #     custom_reward -= 0.05  # 原来是 -0.21，太重！

            # 5. SP 使用效率
            sp_used = self.prev_player_sp - curr_agent_sp
            if sp_used > 0 and damage_dealt == 0:
                custom_reward -= 0.3  # 浪费技能惩罚


            self.prev_player_health = curr_agent_hp
            self.prev_oppont_health = curr_enemy_hp
            self.prev_player_sp = curr_agent_sp
            self.prev_oppont_sp = curr_enemy_sp

            

            # if custom_reward != 0:
            #     print(f'Trainning: custom_reward={custom_reward}')
            custom_done = False
        
        # When reset_round flag is set to False (never reset), the session should always keep going.
        if not self.reset_round:
            custom_done = False

        if self.COST_SWITCH_LIANXU == True:
            self.totalScoreLianXuCount += 1

        if self.totalScoreLianXuCount >= COST_TOTALSCORELIANXUCOUNT:
            self.totalScoreLianXuCount = 0
            self.COST_SWITCH_LIANXU = False;
        
        # Max reward is 6 * full_hp = 1054 (damage * 3 + winning_reward * 3) norm_coefficient = 0.001
        return self._stack_observation(), custom_reward, custom_done, info # reward normalization